{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import scanpy as sc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import geomloss\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "from model.stDiff_model import DiT_stDiff\n",
    "from utils import save_log_file, save_model, SinkhornLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python train_scDYff_DiT.py --dataset DR --ntps 11 --batch_size 1024\n",
    "python train_scDYff_DiT.py --dataset EB --ntps 5 --batch_size 2048\n",
    "python train_scDYff_DiT.py --dataset MB --ntps 13 --batch_size 1024\n",
    "python train_scDYff_DiT.py --dataset MP --ntps 4 --batch_size 2048\n",
    "python train_scDYff_DiT.py --dataset ZB --ntps 12 --batch_size 512\n",
    "\n",
    "'''\n",
    "\n",
    "para_dict = {\n",
    "    'DR': (11, 1024, [4,6,8]),\n",
    "    'EB': (5, 2048, [2,]),\n",
    "    'MB': (13, 1024, [4,6,8]),\n",
    "    'MP': (4, 2048, [2,]),\n",
    "    'ZB': (12, 512, [4,6,8]),\n",
    "}\n",
    "\n",
    "\n",
    "dataset_name = 'DR'\n",
    "dataset_ntps = para_dict[dataset_name][0]\n",
    "batch_size = para_dict[dataset_name][1]\n",
    "test_label_list = para_dict[dataset_name][2]\n",
    "label_list = list(range(dataset_ntps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_num = 50\n",
    "depth = 6\n",
    "hidden_size = 512\n",
    "head = 16\n",
    "\n",
    "\n",
    "model = DiT_stDiff(\n",
    "    input_size=gene_num * 2,\n",
    "    output_size=gene_num,\n",
    "    hidden_size=hidden_size,\n",
    "    depth=depth,\n",
    "    num_heads=head,\n",
    "    classes=6,\n",
    "    dit_type='dit',\n",
    "    mlp_ratio=4.0,\n",
    ")\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "model.to(device)\n",
    "\n",
    "# 加载模型\n",
    "epochs = 1400\n",
    "model_path = f'/home/hanyuji/Results/scDYff/{dataset_name}/model_inter_{dataset_name}_{epochs}epochs.pt'\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = (\n",
    "    f'/home/hanyuji/Results/VAE_result/data_latent/{dataset_name}_latent_50.pkl'\n",
    ")\n",
    "\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    data_list = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = {}\n",
    "for test_tp in test_label_list:\n",
    "    t1 = test_tp - 1\n",
    "    t3 = test_tp + 1\n",
    "    \n",
    "    x1 = data_list[t1]\n",
    "    x3 = data_list[t3]\n",
    "    cell_idx_1 = np.random.choice(\n",
    "        np.arange(x1.shape[0]), size=batch_size, replace=(x1.shape[0] < batch_size)\n",
    "    )\n",
    "    cell_idx_3 = np.random.choice(\n",
    "        np.arange(x3.shape[0]), size=batch_size, replace=(x3.shape[0] < batch_size)\n",
    "    )\n",
    "    x1 = x1[cell_idx_1, :]\n",
    "    x3 = x3[cell_idx_3, :]\n",
    "    x13 = np.concatenate([x1, x3], axis=1)  # batchsize, 100\n",
    "\n",
    "\n",
    "    x13 = torch.tensor(x13).type(torch.float32).to(device)\n",
    "    \n",
    "    t = (test_tp-t1) / (t3-t1)\n",
    "    t = torch.tensor(np.full(x1.shape[0], t)).type(torch.float32).to(device)\n",
    "    \n",
    "    \n",
    "    x2_pred = model(x13, t=t)\n",
    "    pred[test_tp] = x2_pred.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = f'/home/hanyuji/Results/scDYff/interpolation_latent/{dataset_name}_result_dict_50_latent.pt'\n",
    "with open(result_path, 'wb') as f:\n",
    "    pickle.dump(pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ot: 26.12495231628418, l2: 9.66082114150935, cos: 0.9644107625787834, corr: 0.9648374750487079\n",
      "ot: 24.935546875, l2: 9.82725093060234, cos: 0.9872741440194381, corr: 0.9870808272993952\n",
      "ot: 25.15021514892578, l2: 9.947431277149597, cos: 0.9872738788033648, corr: 0.9870056809217979\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# 评估结果\n",
    "for i in test_label_list:\n",
    "    x_pred = pred[i]\n",
    "    x_pred = data_list[i + 1]\n",
    "    x_true = data_list[i]\n",
    "\n",
    "    l2_dist = cdist(x_true, x_pred, metric=\"euclidean\")\n",
    "    cos_dist = cdist(x_true, x_pred, metric=\"cosine\")\n",
    "    corr_dist = cdist(x_true, x_pred, metric=\"correlation\")\n",
    "    avg_l2 = l2_dist.sum() / np.prod(l2_dist.shape)\n",
    "    avg_cos = cos_dist.sum() / np.prod(cos_dist.shape)\n",
    "    avg_corr = corr_dist.sum() / np.prod(corr_dist.shape)\n",
    "\n",
    "\n",
    "    ot = SinkhornLoss(\n",
    "        torch.tensor(x_pred).type(torch.float32).to(device),\n",
    "        torch.tensor(x_true).type(torch.float32).to(device),\n",
    "    ).item()\n",
    "    # l2 = nn.MSELoss(x_pred, x_true)\n",
    "    print(f'ot: {ot}, l2: {avg_l2}, cos: {avg_cos}, corr: {avg_corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([4, 6, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(result_path, 'rb') as f:\n",
    "    loaded_dict = pickle.load(f)\n",
    "\n",
    "loaded_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DYffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
